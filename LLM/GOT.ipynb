{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model='llama3' created_at='2025-01-18T09:51:49.219615057Z' done=True done_reason='stop' total_duration=470790377 load_duration=3145643 prompt_eval_count=None prompt_eval_duration=13555000 eval_count=14 eval_duration=120915000 message=Message(role='assistant', content='\"ç”Ÿæ´»æ˜¯ä¸€åœºæŽ¢é™©\" (Life is an adventure)', images=None, tool_calls=None)\n"
     ]
    }
   ],
   "source": [
    "import ollama \n",
    "# æ™®é€šè¾“å‡ºï¼ˆè¯·å…ˆæŒ‰ç…§å‡†å¤‡å·¥ä½œä¸­çš„è¦æ±‚å®‰è£…æ¨¡åž‹ï¼‰\n",
    "back = ollama.chat(model=\"llama3\",messages=[{\"role\": \"user\",\"content\": \"ç”Ÿæˆä¸€å¥ç®€çŸ­çš„è¯\"}],\n",
    "\t\t\t\t\tstream = False, # æ˜¯å¦æµå¼è¾“å‡º\n",
    "\t\t\t\t\t)\n",
    "print(back) \n",
    "# #æµå¼è¾“å‡º\n",
    "# back = ollama.chat(model=\"llama3\",messages=[{\"role\": \"user\",\"content\": \"ç”Ÿæˆä¸€å¥ç®€çŸ­çš„è¯\"}],\n",
    "#                    stream = True, # æ˜¯å¦æµå¼è¾“å‡º\n",
    "#                    )\n",
    "# for i in back:\n",
    "#     print(back,end = \"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A question that has puzzled humans for centuries! ðŸŒŒ\n",
      "\n",
      "The short answer is: the sky appears blue because of a phenomenon called Rayleigh scattering, named after the British physicist Lord Rayleigh.\n",
      "\n",
      "Here's what happens:\n",
      "\n",
      "1. When sunlight enters Earth's atmosphere, it encounters tiny molecules of gases such as nitrogen (N2) and oxygen (O2).\n",
      "2. These molecules scatter the light in all directions, but they scatter shorter wavelengths (like blue and violet) more than longer wavelengths (like red and orange). This is because the smaller molecules are more effective at scattering the shorter wavelengths.\n",
      "3. As a result, the blue and violet light is distributed evenly throughout the sky, giving it its characteristic blue color.\n",
      "\n",
      "There are some additional factors that contribute to the sky's blue appearance:\n",
      "\n",
      "* Atmospheric particles: Tiny particles in the air, like dust, pollen, and water vapor, can also scatter light and affect its color. However, these particles tend to scatter shorter wavelengths even more than longer ones, making the sky appear even bluer.\n",
      "* Scattering at different angles: When sunlight enters the atmosphere at a shallow angle (like during sunrise or sunset), it has to travel farther through the atmosphere, which means it encounters more molecules and particles. This can make the sky appear more red or orange due to the increased scattering of longer wavelengths.\n",
      "\n",
      "It's worth noting that the exact shade of blue can vary depending on several factors, such as:\n",
      "\n",
      "* The amount of dust and pollutants in the air\n",
      "* The time of day (with sunrise and sunset often appearing more reddish)\n",
      "* The altitude and atmospheric conditions (e.g., high-altitude clouds can make the sky appear more gray or white)\n",
      "\n",
      "So, to summarize: the blue color of the sky is primarily due to Rayleigh scattering, with some additional contributions from atmospheric particles and the angle at which sunlight enters the atmosphere. ðŸŒŠ\n"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "response = ollama.chat(model='llama3', messages=[\n",
    "  {\n",
    "    'role': 'user',\n",
    "    'content': 'ä¸ºä»€ä¹ˆå¤©ç©ºæ˜¯è“è‰²çš„ï¼Ÿ',\n",
    "  },\n",
    "])\n",
    "print(response['message']['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ListResponse(models=[Model(model='llama3:latest', modified_at=datetime.datetime(2025, 1, 13, 19, 3, 22, 824695, tzinfo=TzInfo(+08:00)), digest='365c0bd3c000a25d28ddbf732fe1c6add414de7275464c4e4d1c3b5fcb5d8ad1', size=4661224676, details=ModelDetails(parent_model='', format='gguf', family='llama', families=['llama'], parameter_size='8.0B', quantization_level='Q4_0'))])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ollama.list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Collecting modelscope\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/94/34/cc9faf34851d4b75fb94a9c5748ff6c67cc55c1bfb44014e638a2007dc02/modelscope-1.22.3-py3-none-any.whl (5.8 MB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests>=2.25 in /sda/limingxin/.conda/envs/llm/lib/python3.9/site-packages (from modelscope) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.64.0 in /sda/limingxin/.conda/envs/llm/lib/python3.9/site-packages (from modelscope) (4.67.1)\n",
      "Requirement already satisfied: urllib3>=1.26 in /sda/limingxin/.conda/envs/llm/lib/python3.9/site-packages (from modelscope) (1.26.20)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /sda/limingxin/.conda/envs/llm/lib/python3.9/site-packages (from requests>=2.25->modelscope) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /sda/limingxin/.conda/envs/llm/lib/python3.9/site-packages (from requests>=2.25->modelscope) (3.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /sda/limingxin/.conda/envs/llm/lib/python3.9/site-packages (from requests>=2.25->modelscope) (2024.8.30)\n",
      "Installing collected packages: modelscope\n",
      "Successfully installed modelscope-1.22.3\n"
     ]
    }
   ],
   "source": [
    "!pip install modelscope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modelscope import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name = \"/sdb/ccfa/video_understanding/Qwen2.5-7B-Instruct\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"cuda:0\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Give me a short introduction to large language model.\"\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "import torch\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "generated_ids = model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=512\n",
    ")\n",
    "generated_ids = [\n",
    "    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "]\n",
    "\n",
    "response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/sda/limingxin/.conda/envs/llm/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:09<00:00,  2.50s/it]\n"
     ]
    }
   ],
   "source": [
    "from modelscope import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name = \"/sdb/ccfa/video_understanding/Qwen2.5-7B-Instruct\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"cuda:0\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 3: score\n",
      "æ ¹æ®ä½ æä¾›çš„é¢˜ç›®å’Œç­”æ¡ˆï¼Œæˆ‘é€‰æ‹©çš„ç­”æ¡ˆæ˜¯b) flail aroundã€‚ä½†æ˜¯éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œè¿™ä¸ªç­”æ¡ˆé€‰é¡¹å¹¶ä¸æ˜¯åŸºäºŽé¢˜ç›®ä¸­çš„æè¿°ï¼Œè€Œæ˜¯ä½ æä¾›çš„å¯èƒ½ç­”æ¡ˆä¹‹ä¸€ã€‚é¢˜ç›®ä¸­å¹¶æ²¡æœ‰æåˆ°è§†é¢‘çš„ç»“å°¾éƒ¨åˆ†å‘ç”Ÿäº†ä»€ä¹ˆï¼Œæ‰€ä»¥æ— æ³•ç›´æŽ¥å¾—å‡ºæ­£ç¡®ç­”æ¡ˆã€‚å¦‚æžœb) flail aroundæ˜¯æ­£ç¡®çš„ç­”æ¡ˆï¼Œé‚£ä¹ˆå®ƒåº”è¯¥æ˜¯ä»Žè§†é¢‘ä¸­è§‚å¯Ÿåˆ°çš„å®žé™…è¡Œä¸ºã€‚\n"
     ]
    }
   ],
   "source": [
    "question = \"what did the baby do after he turned around near the end of the videoï¼Ÿ\"\n",
    "prompt = \"There is a little boy, a young child and his mom. The little boy is playing with a toy and wearing a tie. The young child is also playing with a toy. And, the mom is taking care of the little boy and young child.\"\n",
    "fact = \"a) hug boy\tb) flail around\tc) touch the camera\td) push box to where he started\te) fall\"\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"ä½ æ˜¯ä¸€ä¸ªæè¿°é—®ç­”æŽ¨ç†å™¨ï¼Œè¯·æ ¹æ®æˆ‘ç»™ä½ çš„é¢˜ç›®å’Œç­”æ¡ˆï¼Œç»™å‡ºä½ é€‰æ‹©å“ªä¸€ä¸ªã€‚\"},\n",
    "    # {\"role\": \"assistant\", \"content\": \"æˆ‘ç»™ä½ çš„æè¿°ä¸€å®šæ˜¯å’Œé¢˜ç›®ç›¸å…³çš„ï¼Œä½ å¿…é¡»ç»™å‡ºä¸€ä¸ªé€‰é¡¹ã€‚\"},\n",
    "    {\"role\": \"user\", \"content\": \"The scenario isï¼š\"+prompt+\"The question is\"+question+\". The answer is \"+fact},\n",
    "]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "generated_ids = model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=512\n",
    ")\n",
    "generated_ids = [\n",
    "    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "]\n",
    "\n",
    "response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "print('step 3: score')\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 3: score\n",
      "é‡å†™åœºæ™¯æè¿°ï¼šåœ¨è¿™ä¸ªåœºæ™¯ä¸­ï¼Œæœ‰ä¸€ä¸ªå°ç”·å­©å’Œä»–çš„å¦ˆå¦ˆã€‚å°ç”·å­©æ­£åœ¨çŽ©ä¸€ä¸ªçŽ©å…·å¹¶ä¸”ç³»ç€é¢†å¸¦ã€‚å¦ä¸€ä¸ªå°å­©ä¹Ÿåœ¨çŽ©çŽ©å…·ã€‚è€Œå¦ˆå¦ˆæ­£åœ¨ç…§é¡¾è¿™ä¸¤ä¸ªå­©å­ã€‚é—®é¢˜æ˜¯åœ¨è§†é¢‘çš„ç»“å°¾éƒ¨åˆ†ï¼Œå½“å°ç”·å­©è½¬è¿‡èº«ä¹‹åŽï¼Œä»–åšäº†ä»€ä¹ˆï¼Ÿ\n"
     ]
    }
   ],
   "source": [
    "question = \"what did the baby do after he turned around near the end of the videoï¼Ÿ\"\n",
    "prompt = \"There is a little boy, a young child and his mom. The little boy is playing with a toy and wearing a tie. The young child is also playing with a toy. And, the mom is taking care of the little boy and young child.\"\n",
    "fact = \"e) fall\"\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"ä½ æ˜¯ä¸€ä¸ªè¯è¯­ä¿®æ”¹å™¨ï¼Œæˆ‘ç»™ä½ çš„é—®é¢˜å’Œåœºæ™¯æè¿°å¯èƒ½å­˜åœ¨è¿‘ä¹‰è¯çš„é—®é¢˜ï¼Œè¯·ä½ è¿›è¡ŒåŒ¹é…ï¼Œå¹¶å¯¹åœºæ™¯æè¿°è¿›è¡Œè¯è¯­æ›¿æ¢ã€‚\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"ä½ éœ€è¦å›žå¤ä¸€æ®µè¯ï¼Œæ˜¯æˆ‘ç»™ä½ çš„scenarioçš„é‡å†™å†…å®¹\"},\n",
    "    {\"role\": \"user\", \"content\": \"The scenario isï¼š\"+prompt+\" The question is\"+question}\n",
    "]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "generated_ids = model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=512\n",
    ")\n",
    "generated_ids = [\n",
    "    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "]\n",
    "\n",
    "response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "print('step 3: score')\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"entities\": [\n",
      "    {\n",
      "      \"text\": \"baby\",\n",
      "      \"type\": \"PERSON\"\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "sss = \"\"\"'''json\n",
    "{\n",
    "  \"entities\": [\n",
    "    {\n",
    "      \"text\": \"baby\",\n",
    "      \"type\": \"PERSON\"\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "'''\"\"\"\n",
    "\n",
    "prompt = \"Give me a short introduction to large language model.\"\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"ä½ æ˜¯ä¸€ä¸ªå®žä½“æŠ½å–å’Œå¥æ³•ä¾å­˜åˆ†æžå™¨ï¼Œè¯·åŸºäºŽé—®é¢˜ï¼Œç»™å‡ºåœºæ™¯ä¸­çš„å®žä½“ï¼Œå¹¶å’Œé¢˜ç›®ä¸­çš„å®žä½“è¿›è¡ŒåŒ¹é…ï¼Œç»™å‡ºå®žä½“å†…å®¹ï¼Œä»¥é—®å¥ä¸­çš„å®žä½“ä¸ºå‡†ï¼Œç»™å‡ºå¯¹åº”çš„jsonæ ¼å¼ã€‚ä½ åªèƒ½ç»™å‡ºå®žä½“ï¼Œæ ¼å¼å¦‚ä¸‹ï¼š\"+sss},\n",
    "    {\"role\": \"assistant\", \"content\": \"what did the baby do after he turned around near the end of the videoï¼Ÿ\"},\n",
    "    {\"role\": \"user\", \"content\": \"The scenario isï¼šThere is a little boy, a young child and his mom. The little boy is playing with a toy and wearing a tie. The young child is also playing with a toy. And, the mom is taking care of the little boy and young child.\"},\n",
    "]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "import torch\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "generated_ids = model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=512\n",
    ")\n",
    "generated_ids = [\n",
    "    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "]\n",
    "\n",
    "response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "responses = []\n",
    "responses.append(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To answer this question, let's break down the given information and analyze the scene step by step.\n",
      "\n",
      "1. **Scene Description:**\n",
      "   - There are three individuals: a little boy, a young child (presumably the same as the little boy), and their mother.\n",
      "   - The little boy is playing with a toy and is wearing a tie.\n",
      "   - The young child is also playing with a toy.\n",
      "   - The mother is taking care of both children.\n",
      "\n",
      "2. **Question Analysis:**\n",
      "   - We need to determine what the baby (presumably the young child) did after turning around near the end of the video.\n",
      "\n",
      "3. **Logical Inference:**\n",
      "   - Since no specific actions are mentioned for the baby after turning around, we must make an inference based on typical behaviors of young children.\n",
      "   - Young children often engage in various activities after turning around, such as looking at something new, reaching for a toy, or making sounds.\n",
      "\n",
      "4. **Scene Relationship Diagram:**\n",
      "   - Little Boy: Playing with a toy, wearing a tie\n",
      "   - Young Child: Playing with a toy\n",
      "   - Mother: Taking care of both children\n",
      "   - Interaction: The young child turns around and does something typical for a young child.\n",
      "\n",
      "5. **JSON Format:**\n",
      "```json\n",
      "{\n",
      "  \"scene\": {\n",
      "    \"characters\": [\n",
      "      {\n",
      "        \"name\": \"Little Boy\",\n",
      "        \"action\": \"Playing with a toy, wearing a tie\"\n",
      "      },\n",
      "      {\n",
      "        \"name\": \"Young Child\",\n",
      "        \"action\": \"Playing with a toy\"\n",
      "      },\n",
      "      {\n",
      "        \"name\": \"Mother\",\n",
      "        \"action\": \"Taking care of both children\"\n",
      "      }\n",
      "    ],\n",
      "    \"interaction\": {\n",
      "      \"description\": \"The young child turns around and engages in a typical child behavior such as looking at something new, reaching for a toy, or making sounds.\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "```\n",
      "\n",
      "This JSON format encapsulates the characters, their actions, and the inferred interaction based on the given scenario.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Give me a short introduction to large language model.\"\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"ä½ åªèƒ½ç”¨è‹±è¯­å›žå¤ï¼Œä½ æ˜¯ä¸€ä¸ªåœºæ™¯å†…å®¹æŽ¨ç†å™¨ï¼Œè¯·åŸºäºŽåœºæ™¯å’Œå†…å®¹ï¼Œç»™å‡ºæŽ¨ç†è¿‡ç¨‹ï¼Œç»™å‡ºåœºæ™¯å…³ç³»å›¾ï¼Œç»™å‡ºjsonæ ¼å¼ã€‚\"},\n",
    "    {\"role\": \"user\", \"content\": \"The description isï¼šThere is a little boy, a young child and his mom. The little boy is playing with a toy and wearing a tie. The young child is also playing with a toy. And, the mom is taking care of the little boy and young child. what did the baby do after he turned around near the end of the videoï¼Ÿ \"},\n",
    "]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "import torch\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "generated_ids = model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=1024\n",
    ")\n",
    "generated_ids = [\n",
    "    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "]\n",
    "\n",
    "response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "print(response)\n",
    "\n",
    "{\"role\": \"assistant\", \"content\": f\"The question is {question}. The options are {A B C D E}. ç»™å‡ºé€‰æ‹©è¿™ä¸ªé€‰é¡¹çš„å¯èƒ½çš„æ¦‚çŽ‡ï¼Œä»Ž1åˆ°10ç»™å‡ºè¯„åˆ†ï¼Œ10ä¸ºæœ€æœ‰å¯èƒ½ï¼Œ1ä¸ºæ²¡æœ‰å¯èƒ½ã€‚æ ¼å¼ä¸ºã€1/10ã€‘\"},"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = response\n",
    "import re\n",
    "import json\n",
    "flag = 0\n",
    "# text = text2\n",
    "try:\n",
    "    json_pattern = r\"'''json\\s*(\\{(?:.|\\n)*?\\})\\s*'''\"\n",
    "    matches = re.findall(json_pattern, text)\n",
    "    # print(matches)\n",
    "    # å‡è®¾æˆ‘ä»¬åªå…³å¿ƒç¬¬ä¸€ä¸ªåŒ¹é…é¡¹\n",
    "    json_data = json.loads(matches[0]) if matches else None\n",
    "    print(json_data)\n",
    "except:\n",
    "    flag = 1\n",
    "if flag:\n",
    "  try:\n",
    "      json_pattern = r'\\{.*\\}'\n",
    "      matches = re.findall(json_pattern, text2, re.DOTALL)\n",
    "\n",
    "      # å‡è®¾æˆ‘ä»¬åªå…³å¿ƒç¬¬ä¸€ä¸ªåŒ¹é…é¡¹\n",
    "      json_data = json.loads(matches[0]) if matches else None\n",
    "      print(json_data)\n",
    "  except:\n",
    "      flag = 101"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_entity(prompt, question):\n",
    "    sss = \"\"\"'''json\n",
    "    {\n",
    "    \"entities\": [\n",
    "        {\n",
    "        \"text\": \"baby\",\n",
    "        \"type\": \"PERSON\"\n",
    "        }\n",
    "    ]\n",
    "    }\n",
    "    '''\"\"\"\n",
    "        \n",
    "    prompt = \"Give me a short introduction to large language model.\"\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"ä½ æ˜¯ä¸€ä¸ªå®žä½“æŠ½å–å’Œå¥æ³•ä¾å­˜åˆ†æžå™¨ï¼Œè¯·åŸºäºŽé—®é¢˜ï¼Œç»™å‡ºåœºæ™¯ä¸­çš„å®žä½“ï¼Œå¹¶å’Œé¢˜ç›®ä¸­çš„å®žä½“è¿›è¡ŒåŒ¹é…ï¼Œç»™å‡ºå®žä½“å†…å®¹ï¼Œä»¥é—®å¥ä¸­çš„å®žä½“ä¸ºå‡†ï¼Œç»™å‡ºå¯¹åº”çš„jsonæ ¼å¼ã€‚ä½ åªèƒ½ç»™å‡ºå®žä½“ï¼Œæ ¼å¼å¦‚ä¸‹ï¼š\"+sss},\n",
    "        {\"role\": \"assistant\", \"content\": question},\n",
    "        {\"role\": \"user\", \"content\": \"The scenario isï¼š\"+prompt},\n",
    "    ]\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    generated_ids = model.generate(\n",
    "        **model_inputs,\n",
    "        max_new_tokens=512\n",
    "    )\n",
    "    generated_ids = [\n",
    "        output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "    ]\n",
    "\n",
    "    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "    print('step 1: entity')\n",
    "    print(response)\n",
    "\n",
    "    return response\n",
    "\n",
    "def generate_scene_graph(prompt, question):\n",
    "    messages = [\n",
    "    {\"role\": \"system\", \"content\": \"ä½ åªèƒ½ç”¨è‹±è¯­å›žå¤ï¼Œä½ æ˜¯ä¸€ä¸ªåœºæ™¯å†…å®¹æŽ¨ç†å™¨ï¼Œè¯·åŸºäºŽåœºæ™¯å’Œå†…å®¹ï¼Œç»™å‡ºæŽ¨ç†è¿‡ç¨‹ï¼Œç»™å‡ºåœºæ™¯å…³ç³»å›¾ï¼Œç»™å‡ºjsonæ ¼å¼ã€‚\"},\n",
    "    {\"role\": \"user\", \"content\": \"The description isï¼šThere is a little boy, a young child and his mom. The little boy is playing with a toy and wearing a tie. The young child is also playing with a toy. And, the mom is taking care of the little boy and young child. what did the baby do after he turned around near the end of the videoï¼Ÿ \"},\n",
    "]\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    generated_ids = model.generate(\n",
    "        **model_inputs,\n",
    "        max_new_tokens=1024\n",
    "    )\n",
    "    generated_ids = [\n",
    "        output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "    ]\n",
    "\n",
    "    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "\n",
    "    text = response\n",
    "    import re\n",
    "    import json\n",
    "    flag = 0\n",
    "    # text = text2\n",
    "    try:\n",
    "        json_pattern = r\"'''json\\s*(\\{(?:.|\\n)*?\\})\\s*'''\"\n",
    "        matches = re.findall(json_pattern, text)\n",
    "        # print(matches)\n",
    "        # å‡è®¾æˆ‘ä»¬åªå…³å¿ƒç¬¬ä¸€ä¸ªåŒ¹é…é¡¹\n",
    "        json_data = json.loads(matches[0]) if matches else None\n",
    "        print(json_data)\n",
    "    except:\n",
    "        flag = 1\n",
    "        if flag:\n",
    "            try:\n",
    "                json_pattern = r'\\{.*\\}'\n",
    "                matches = re.findall(json_pattern, text, re.DOTALL)\n",
    "\n",
    "                # å‡è®¾æˆ‘ä»¬åªå…³å¿ƒç¬¬ä¸€ä¸ªåŒ¹é…é¡¹\n",
    "                json_data = json.loads(matches[0]) if matches else None\n",
    "                print(json_data)\n",
    "            except:\n",
    "                flag = 101\n",
    "    print('step 2: scene graph')\n",
    "    print(json_data)\n",
    "    return json_data\n",
    "\n",
    "def generate_enhancement_scene(prompt, question, fact):\n",
    "    # prompt = \"Give me a short introduction to large language model.\"\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"ä½ æ˜¯ä¸€ä¸ªåœºæ™¯å¢žå¼ºå™¨ï¼Œä½ éœ€è¦æ ¹æ®æˆ‘çš„é—®é¢˜å’Œæˆ‘ç»™ä½ çš„ç­”æ¡ˆè¿˜æœ‰å¢žå¼ºåœºæ™¯æè¿°ï¼Œå°†è¿™å¥è¯æ‰©å±•åˆ°ä½ ç”Ÿæˆçš„æè¿°ä¸­ã€‚\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"ç”¨è‹±è¯­å›žç­”ï¼Œåªèƒ½ç”Ÿæˆä¸€æ®µè¯\"},\n",
    "        {\"role\": \"user\", \"content\": \"The scenario isï¼š\"+prompt+\"The question is\"+question+\"The answer (fact) is\"+fact},\n",
    "    ]\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    generated_ids = model.generate(\n",
    "        **model_inputs,\n",
    "        max_new_tokens=512\n",
    "    )\n",
    "    generated_ids = [\n",
    "        output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "    ]\n",
    "\n",
    "    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "    print('step 1: entity')\n",
    "    print(response)\n",
    "\n",
    "    return response\n",
    "\n",
    "def generate_question_rank(prompt, question, fact):\n",
    "    # prompt = \"Give me a short introduction to large language model.\"\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"ä½ æ˜¯ä¸€ä¸ªé¢˜ç›®ä¸Žåœºæ™¯æ¯”å¯¹å™¨ï¼Œæ ¹æ®æˆ‘ç»™ä½ çš„åœºæ™¯æè¿°ï¼Œä¸€æ­¥ä¸€æ­¥åˆ†æž.\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"ç”¨è‹±è¯­å›žç­”ï¼Œä»Ž0-10è¿›è¡Œæ‰“åˆ†ï¼Œç»™å‡ºä½ è®¤ä¸ºçš„ç›¸å…³æ€§ï¼Œå®Œå…¨åˆç†èµ‹10åˆ†ï¼Œå®Œå…¨ä¸åˆç†èµ‹0åˆ†ã€‚è¾“å‡ºæ ¼å¼ï¼š[5]ï¼Œä½ çš„å›žç­”æ˜¯ä»€ä¹ˆã€‚\"},\n",
    "        {\"role\": \"user\", \"content\": \"The scenario isï¼š\"+prompt+\"The question is\"+question+\". The answer is \"+fact},\n",
    "    ]\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    generated_ids = model.generate(\n",
    "        **model_inputs,\n",
    "        max_new_tokens=512\n",
    "    )\n",
    "    generated_ids = [\n",
    "        output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "    ]\n",
    "\n",
    "    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "    print('step 3: score')\n",
    "    print(response)\n",
    "    \n",
    "    return response\n",
    "\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:04<00:00,  1.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æž„å»ºscene graphï¼š\n",
      "- ä¸»ä½“1ï¼ˆperson1ï¼‰ï¼šå°ç”·å­©ï¼ˆlittle boyï¼‰ï¼Œç‰¹å¾ï¼šå¹´è½»ï¼Œæ­£åœ¨çŽ©çŽ©å…·ï¼Œç©¿ç€é¢†ç»“\n",
      "- ä¸»ä½“2ï¼ˆperson2ï¼‰ï¼šå¹´å¹¼çš„å­©å­ï¼ˆyoung childï¼‰ï¼Œç‰¹å¾ï¼šå¹´è½»ï¼Œæ­£åœ¨çŽ©çŽ©å…·\n",
      "- ä¸»ä½“3ï¼ˆperson3ï¼‰ï¼šå¦ˆå¦ˆï¼ˆmomï¼‰ï¼Œç‰¹å¾ï¼šåœ¨ç…§é¡¾å°ç”·å­©å’Œå¹´å¹¼çš„å­©å­\n",
      "- ç‰©å“1ï¼ˆobject1ï¼‰ï¼šçŽ©å…·ï¼ˆtoyï¼‰\n",
      "- ç‰©å“2ï¼ˆobject2ï¼‰ï¼šé¢†ç»“ï¼ˆtieï¼‰\n",
      "\n",
      "åˆ†æžé€‰é¡¹å¹¶è¯„åˆ†ï¼š\n",
      "\n",
      "a) hug boy (æ‹¥æŠ±ç”·å­©)\n",
      "   - åˆ†æžï¼šè§†é¢‘ä¸­æ²¡æœ‰æåˆ°å°ç”·å­©è¢«å…¶ä»–äººæ‹¥æŠ±ï¼Œä¹Ÿæ²¡æœ‰ä»»ä½•ä¿¡æ¯è¡¨æ˜Žä»–ä¼šåŽ»æ‹¥æŠ±åˆ«äººã€‚\n",
      "   - è¯„åˆ†ï¼š1\n",
      "\n",
      "b) flail around (ä¹±åŠ¨)\n",
      "   - åˆ†æžï¼šè§†é¢‘ä¸­æ²¡æœ‰æ˜Žç¡®è¯´æ˜Žå°ç”·å­©æ˜¯å¦ä¼šæœ‰è¿™ç§è¡Œä¸ºï¼Œä½†è€ƒè™‘åˆ°ä»–æ­£åœ¨çŽ©çŽ©å…·ï¼Œæœ‰å¯èƒ½ä¼šå‡ºçŽ°è¿™ç§æƒ…å†µã€‚\n",
      "   - è¯„åˆ†ï¼š4\n",
      "\n",
      "c) touch the camera (è§¦ç¢°ç›¸æœº)\n",
      "   - åˆ†æžï¼šåœºæ™¯æè¿°ä¸­å¹¶æ²¡æœ‰æåˆ°ç›¸æœºçš„å­˜åœ¨ï¼Œå› æ­¤è¿™ä¸ªé€‰é¡¹ä¸ç¬¦åˆåœºæ™¯æè¿°ã€‚\n",
      "   - è¯„åˆ†ï¼š1\n",
      "\n",
      "d) push box to where he started (æŠŠç®±å­æŽ¨åˆ°ä»–å¼€å§‹çš„åœ°æ–¹)\n",
      "   - åˆ†æžï¼šåœºæ™¯æè¿°ä¸­å¹¶æ²¡æœ‰æåˆ°ç®±å­ï¼Œæ‰€ä»¥è¿™ä¸ªé€‰é¡¹ä¹Ÿä¸ç¬¦åˆåœºæ™¯æè¿°ã€‚\n",
      "   - è¯„åˆ†ï¼š1\n",
      "\n",
      "e) fall (æ‘”å€’)\n",
      "   - åˆ†æžï¼šè§†é¢‘ä¸­æ²¡æœ‰æåˆ°å°ç”·å­©æ‘”å€’çš„æƒ…å†µï¼Œä¹Ÿæ²¡æœ‰ä»»ä½•ä¿¡æ¯è¡¨æ˜Žä»–ä¼šè¿™æ ·åšã€‚\n",
      "   - è¯„åˆ†ï¼š1\n",
      "\n",
      "æœ€ç»ˆç­”æ¡ˆï¼šæ ¹æ®ä»¥ä¸Šåˆ†æžï¼Œæœ€ç¬¦åˆåœºæ™¯æè¿°çš„é€‰é¡¹æ˜¯b) flail around (ä¹±åŠ¨)ï¼Œè¯„åˆ†ä¸º4ã€‚ç„¶è€Œï¼Œéœ€è¦æ³¨æ„çš„æ˜¯ï¼Œè¿™åªæ˜¯ä¸€ä¸ªæŽ¨æµ‹æ€§çš„ç»“è®ºï¼Œå› ä¸ºé¢˜å¹²ä¸­çš„æè¿°å¹¶ä¸å®Œæ•´ï¼Œæ— æ³•å®Œå…¨ç¡®å®šå°ç”·å­©çš„è¡Œä¸ºã€‚\n"
     ]
    }
   ],
   "source": [
    "from modelscope import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name = \"/sdb/ccfa/video_understanding/Qwen2.5-7B-Instruct\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"cuda:0\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# history\n",
    "question = \"what did the baby do after he turned around near the end of the videoï¼Ÿ\"\n",
    "prompt = \"There is a little boy, a young child and his mom. The little boy is playing with a toy and wearing a tie. The young child is also playing with a toy. And, the mom is taking care of the little boy and young child.\"\n",
    "fact = \"a) hug boy\tb) flail around\tc) touch the camera\td) push box to where he started\te) fall\"\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"ä½ æ˜¯ä¸€ä¸ªå†…å®¹ç†è§£ä¸ŽæŽ¨ç†ä¸“å®¶ï¼Œä½ éœ€è¦åšçš„æ˜¯ä»Žæˆ‘çš„åœºæ™¯æè¿°ä¸­è¿›è¡Œå†…å®¹ç†è§£å’ŒæŽ¨ç†ï¼Œè¯·åŸºäºŽé—®é¢˜ï¼ŒæŽ¨æ–­å‡ºåœºæ™¯ï¼Œå¹¶å›žç­”æˆ‘çš„é—®é¢˜ã€‚\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"è¯·å›žç­”æˆ‘çš„é—®é¢˜ï¼šThe question is\"+question+\". The options is \"+fact+\". å›žç­”æ ¼å¼ï¼šã€é€‰é¡¹k)ã€‘\"},\n",
    "    {\"role\": \"user\", \"content\": \"è¯·æ ¹æ®æˆ‘ç»™ä½ çš„å†…å®¹ä¸€æ­¥ä¸€æ­¥è€ƒè™‘ï¼š1) æž„å»ºscene graph 2) å¯¹æ¯ä¸ªé€‰é¡¹è¿›è¡Œåˆ†æžï¼Œå¯¹æ¯ä¸ªé€‰é¡¹æ‰“ä¸€ä¸ª1-10çš„è¯„åˆ†ã€‚3ï¼‰ç»™å‡ºæœ€ç»ˆçš„ç­”æ¡ˆã€‚The scenario isï¼š\"+prompt}\n",
    "]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "generated_ids = model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=512\n",
    ")\n",
    "generated_ids = [\n",
    "    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "]\n",
    "\n",
    "response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "# print('step 3: score')\n",
    "print(response)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enhance(prompt, fact):\n",
    "    question = \"what did the baby do after he turned around near the end of the videoï¼Ÿ\"\n",
    "    prompt = \"There is a little boy, a young child and his mom. The little boy is playing with a toy and wearing a tie. The young child is also playing with a toy. And, the mom is taking care of the little boy and young child.\"\n",
    "    fact = \"a) hug boy\tb) flail around\tc) touch the camera\td) push box to where he started\te) fall\"\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"ä½ æ˜¯ä¸€ä¸ªå†…å®¹å¢žå¼ºä¸“å®¶ï¼Œä½ éœ€è¦æ ¹æ®æˆ‘ç»™ä½ çš„äº‹å®žï¼ˆé—®ç­”é¢˜ï¼‰ï¼Œå¯¹åœºæ™¯ä»‹ç»è¿›è¡Œå¢žå¼ºï¼Œå¹¶æœ€åŽåŒæ ·ç”Ÿæˆä¸€æ®µè¯ã€‚\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"question: \"+question+\". answer: \"+fact+\". è¯·ç”Ÿæˆä¸€æ®µå†…å®¹å¢žå¼ºçš„è¯ã€‚\"},\n",
    "        {\"role\": \"user\", \"content\": \"The scenario isï¼š\"+prompt}\n",
    "    ]\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    generated_ids = model.generate(\n",
    "        **model_inputs,\n",
    "        max_new_tokens=512\n",
    "    )\n",
    "    generated_ids = [\n",
    "        output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "    ]\n",
    "\n",
    "    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "    # print('step 3: score')\n",
    "    print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In this heartwarming scene, a little boy, adorned in a charming tie, is joyfully playing with a colorful toy while his attentive mother keeps a watchful eye on him. Nearby, his younger sibling is equally engrossed in their own playtime with a delightful toy. As the video progresses, the little boy, who has been sitting contentedly, decides to turn around, only for the camera to capture a moment of pure, innocent delight â€“ and a gentle reminder of childhood antics â€“ as he falls softly to the ground, all the while his mother's loving gaze never straying far.\n"
     ]
    }
   ],
   "source": [
    "# question = \"what did the baby do after he turned around near the end of the videoï¼Ÿ\"\n",
    "# prompt = \"There is a little boy, a young child and his mom. The little boy is playing with a toy and wearing a tie. The young child is also playing with a toy. And, the mom is taking care of the little boy and young child.\"\n",
    "# fact = \"e) fall\"\n",
    "# messages = [\n",
    "#     {\"role\": \"system\", \"content\": \"ä½ æ˜¯ä¸€ä¸ªå†…å®¹å¢žå¼ºä¸“å®¶ï¼Œä½ éœ€è¦æ ¹æ®æˆ‘ç»™ä½ çš„äº‹å®žï¼ˆé—®ç­”é¢˜ï¼‰ï¼Œå¯¹åœºæ™¯ä»‹ç»è¿›è¡Œå¢žå¼ºï¼Œå¹¶æœ€åŽåŒæ ·ç”Ÿæˆä¸€æ®µè¯ã€‚\"},\n",
    "#     {\"role\": \"assistant\", \"content\": \"question: \"+question+\". answer: \"+fact+\". è¯·ç”Ÿæˆä¸€æ®µå†…å®¹å¢žå¼ºçš„è¯ã€‚\"},\n",
    "#     {\"role\": \"user\", \"content\": \"The scenario isï¼š\"+prompt}\n",
    "# ]\n",
    "# text = tokenizer.apply_chat_template(\n",
    "#     messages,\n",
    "#     tokenize=False,\n",
    "#     add_generation_prompt=True\n",
    "# )\n",
    "# model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "# generated_ids = model.generate(\n",
    "#     **model_inputs,\n",
    "#     max_new_tokens=512\n",
    "# )\n",
    "# generated_ids = [\n",
    "#     output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "# ]\n",
    "\n",
    "# response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "# # print('step 3: score')\n",
    "# print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def response_fact(prompt, question, fact):\n",
    "    question = \"what did the baby do after he turned around near the end of the videoï¼Ÿ\"\n",
    "    prompt = \"There is a little boy, a young child and his mom. The little boy is playing with a toy and wearing a tie. The young child is also playing with a toy. And, the mom is taking care of the little boy and young child.\"\n",
    "    fact = \"a) hug boy\tb) flail around\tc) touch the camera\td) push box to where he started\te) fall\"\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"ä½ æ˜¯ä¸€ä¸ªå†…å®¹ç†è§£ä¸ŽæŽ¨ç†ä¸“å®¶ï¼Œä½ éœ€è¦åšçš„æ˜¯ä»Žæˆ‘çš„åœºæ™¯æè¿°ä¸­è¿›è¡Œå†…å®¹ç†è§£å’ŒæŽ¨ç†ï¼Œè¯·åŸºäºŽé—®é¢˜ï¼ŒæŽ¨æ–­å‡ºåœºæ™¯ï¼Œå¹¶å›žç­”æˆ‘çš„é—®é¢˜ã€‚\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"è¯·å›žç­”æˆ‘çš„é—®é¢˜ï¼šThe question is\"+question+\". The options is \"+fact+\". å›žç­”æ ¼å¼ï¼šã€é€‰é¡¹k)ã€‘\"},\n",
    "        {\"role\": \"user\", \"content\": \"è¯·æ ¹æ®æˆ‘ç»™ä½ çš„å†…å®¹ä¸€æ­¥ä¸€æ­¥è€ƒè™‘ï¼š1) æž„å»ºscene graph 2) å¯¹æ¯ä¸ªé€‰é¡¹è¿›è¡Œåˆ†æžï¼Œå¯¹æ¯ä¸ªé€‰é¡¹æ‰“ä¸€ä¸ª1-10çš„è¯„åˆ†ã€‚3ï¼‰ç»™å‡ºæœ€ç»ˆçš„ç­”æ¡ˆã€‚The scenario isï¼š\"+prompt}\n",
    "    ]\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    generated_ids = model.generate(\n",
    "        **model_inputs,\n",
    "        max_new_tokens=512\n",
    "    )\n",
    "    generated_ids = [\n",
    "        output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "    ]\n",
    "\n",
    "    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "    # print('step 3: score')\n",
    "    print(response)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
