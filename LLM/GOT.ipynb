{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model='llama3' created_at='2025-01-18T09:51:49.219615057Z' done=True done_reason='stop' total_duration=470790377 load_duration=3145643 prompt_eval_count=None prompt_eval_duration=13555000 eval_count=14 eval_duration=120915000 message=Message(role='assistant', content='\"生活是一场探险\" (Life is an adventure)', images=None, tool_calls=None)\n"
     ]
    }
   ],
   "source": [
    "import ollama \n",
    "# 普通输出（请先按照准备工作中的要求安装模型）\n",
    "back = ollama.chat(model=\"llama3\",messages=[{\"role\": \"user\",\"content\": \"生成一句简短的话\"}],\n",
    "\t\t\t\t\tstream = False, # 是否流式输出\n",
    "\t\t\t\t\t)\n",
    "print(back) \n",
    "# #流式输出\n",
    "# back = ollama.chat(model=\"llama3\",messages=[{\"role\": \"user\",\"content\": \"生成一句简短的话\"}],\n",
    "#                    stream = True, # 是否流式输出\n",
    "#                    )\n",
    "# for i in back:\n",
    "#     print(back,end = \"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A question that has puzzled humans for centuries! 🌌\n",
      "\n",
      "The short answer is: the sky appears blue because of a phenomenon called Rayleigh scattering, named after the British physicist Lord Rayleigh.\n",
      "\n",
      "Here's what happens:\n",
      "\n",
      "1. When sunlight enters Earth's atmosphere, it encounters tiny molecules of gases such as nitrogen (N2) and oxygen (O2).\n",
      "2. These molecules scatter the light in all directions, but they scatter shorter wavelengths (like blue and violet) more than longer wavelengths (like red and orange). This is because the smaller molecules are more effective at scattering the shorter wavelengths.\n",
      "3. As a result, the blue and violet light is distributed evenly throughout the sky, giving it its characteristic blue color.\n",
      "\n",
      "There are some additional factors that contribute to the sky's blue appearance:\n",
      "\n",
      "* Atmospheric particles: Tiny particles in the air, like dust, pollen, and water vapor, can also scatter light and affect its color. However, these particles tend to scatter shorter wavelengths even more than longer ones, making the sky appear even bluer.\n",
      "* Scattering at different angles: When sunlight enters the atmosphere at a shallow angle (like during sunrise or sunset), it has to travel farther through the atmosphere, which means it encounters more molecules and particles. This can make the sky appear more red or orange due to the increased scattering of longer wavelengths.\n",
      "\n",
      "It's worth noting that the exact shade of blue can vary depending on several factors, such as:\n",
      "\n",
      "* The amount of dust and pollutants in the air\n",
      "* The time of day (with sunrise and sunset often appearing more reddish)\n",
      "* The altitude and atmospheric conditions (e.g., high-altitude clouds can make the sky appear more gray or white)\n",
      "\n",
      "So, to summarize: the blue color of the sky is primarily due to Rayleigh scattering, with some additional contributions from atmospheric particles and the angle at which sunlight enters the atmosphere. 🌊\n"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "response = ollama.chat(model='llama3', messages=[\n",
    "  {\n",
    "    'role': 'user',\n",
    "    'content': '为什么天空是蓝色的？',\n",
    "  },\n",
    "])\n",
    "print(response['message']['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ListResponse(models=[Model(model='llama3:latest', modified_at=datetime.datetime(2025, 1, 13, 19, 3, 22, 824695, tzinfo=TzInfo(+08:00)), digest='365c0bd3c000a25d28ddbf732fe1c6add414de7275464c4e4d1c3b5fcb5d8ad1', size=4661224676, details=ModelDetails(parent_model='', format='gguf', family='llama', families=['llama'], parameter_size='8.0B', quantization_level='Q4_0'))])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ollama.list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Collecting modelscope\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/94/34/cc9faf34851d4b75fb94a9c5748ff6c67cc55c1bfb44014e638a2007dc02/modelscope-1.22.3-py3-none-any.whl (5.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests>=2.25 in /sda/limingxin/.conda/envs/llm/lib/python3.9/site-packages (from modelscope) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.64.0 in /sda/limingxin/.conda/envs/llm/lib/python3.9/site-packages (from modelscope) (4.67.1)\n",
      "Requirement already satisfied: urllib3>=1.26 in /sda/limingxin/.conda/envs/llm/lib/python3.9/site-packages (from modelscope) (1.26.20)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /sda/limingxin/.conda/envs/llm/lib/python3.9/site-packages (from requests>=2.25->modelscope) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /sda/limingxin/.conda/envs/llm/lib/python3.9/site-packages (from requests>=2.25->modelscope) (3.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /sda/limingxin/.conda/envs/llm/lib/python3.9/site-packages (from requests>=2.25->modelscope) (2024.8.30)\n",
      "Installing collected packages: modelscope\n",
      "Successfully installed modelscope-1.22.3\n"
     ]
    }
   ],
   "source": [
    "!pip install modelscope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modelscope import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name = \"/sdb/ccfa/video_understanding/Qwen2.5-7B-Instruct\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"cuda:0\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Give me a short introduction to large language model.\"\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "import torch\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "generated_ids = model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=512\n",
    ")\n",
    "generated_ids = [\n",
    "    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "]\n",
    "\n",
    "response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/sda/limingxin/.conda/envs/llm/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  2.50s/it]\n"
     ]
    }
   ],
   "source": [
    "from modelscope import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name = \"/sdb/ccfa/video_understanding/Qwen2.5-7B-Instruct\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"cuda:0\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 3: score\n",
      "根据你提供的题目和答案，我选择的答案是b) flail around。但是需要注意的是，这个答案选项并不是基于题目中的描述，而是你提供的可能答案之一。题目中并没有提到视频的结尾部分发生了什么，所以无法直接得出正确答案。如果b) flail around是正确的答案，那么它应该是从视频中观察到的实际行为。\n"
     ]
    }
   ],
   "source": [
    "question = \"what did the baby do after he turned around near the end of the video？\"\n",
    "prompt = \"There is a little boy, a young child and his mom. The little boy is playing with a toy and wearing a tie. The young child is also playing with a toy. And, the mom is taking care of the little boy and young child.\"\n",
    "fact = \"a) hug boy\tb) flail around\tc) touch the camera\td) push box to where he started\te) fall\"\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"你是一个描述问答推理器，请根据我给你的题目和答案，给出你选择哪一个。\"},\n",
    "    # {\"role\": \"assistant\", \"content\": \"我给你的描述一定是和题目相关的，你必须给出一个选项。\"},\n",
    "    {\"role\": \"user\", \"content\": \"The scenario is：\"+prompt+\"The question is\"+question+\". The answer is \"+fact},\n",
    "]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "generated_ids = model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=512\n",
    ")\n",
    "generated_ids = [\n",
    "    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "]\n",
    "\n",
    "response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "print('step 3: score')\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 3: score\n",
      "重写场景描述：在这个场景中，有一个小男孩和他的妈妈。小男孩正在玩一个玩具并且系着领带。另一个小孩也在玩玩具。而妈妈正在照顾这两个孩子。问题是在视频的结尾部分，当小男孩转过身之后，他做了什么？\n"
     ]
    }
   ],
   "source": [
    "question = \"what did the baby do after he turned around near the end of the video？\"\n",
    "prompt = \"There is a little boy, a young child and his mom. The little boy is playing with a toy and wearing a tie. The young child is also playing with a toy. And, the mom is taking care of the little boy and young child.\"\n",
    "fact = \"e) fall\"\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"你是一个词语修改器，我给你的问题和场景描述可能存在近义词的问题，请你进行匹配，并对场景描述进行词语替换。\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"你需要回复一段话，是我给你的scenario的重写内容\"},\n",
    "    {\"role\": \"user\", \"content\": \"The scenario is：\"+prompt+\" The question is\"+question}\n",
    "]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "generated_ids = model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=512\n",
    ")\n",
    "generated_ids = [\n",
    "    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "]\n",
    "\n",
    "response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "print('step 3: score')\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"entities\": [\n",
      "    {\n",
      "      \"text\": \"baby\",\n",
      "      \"type\": \"PERSON\"\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "sss = \"\"\"'''json\n",
    "{\n",
    "  \"entities\": [\n",
    "    {\n",
    "      \"text\": \"baby\",\n",
    "      \"type\": \"PERSON\"\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "'''\"\"\"\n",
    "\n",
    "prompt = \"Give me a short introduction to large language model.\"\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"你是一个实体抽取和句法依存分析器，请基于问题，给出场景中的实体，并和题目中的实体进行匹配，给出实体内容，以问句中的实体为准，给出对应的json格式。你只能给出实体，格式如下：\"+sss},\n",
    "    {\"role\": \"assistant\", \"content\": \"what did the baby do after he turned around near the end of the video？\"},\n",
    "    {\"role\": \"user\", \"content\": \"The scenario is：There is a little boy, a young child and his mom. The little boy is playing with a toy and wearing a tie. The young child is also playing with a toy. And, the mom is taking care of the little boy and young child.\"},\n",
    "]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "import torch\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "generated_ids = model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=512\n",
    ")\n",
    "generated_ids = [\n",
    "    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "]\n",
    "\n",
    "response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "responses = []\n",
    "responses.append(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To answer this question, let's break down the given information and analyze the scene step by step.\n",
      "\n",
      "1. **Scene Description:**\n",
      "   - There are three individuals: a little boy, a young child (presumably the same as the little boy), and their mother.\n",
      "   - The little boy is playing with a toy and is wearing a tie.\n",
      "   - The young child is also playing with a toy.\n",
      "   - The mother is taking care of both children.\n",
      "\n",
      "2. **Question Analysis:**\n",
      "   - We need to determine what the baby (presumably the young child) did after turning around near the end of the video.\n",
      "\n",
      "3. **Logical Inference:**\n",
      "   - Since no specific actions are mentioned for the baby after turning around, we must make an inference based on typical behaviors of young children.\n",
      "   - Young children often engage in various activities after turning around, such as looking at something new, reaching for a toy, or making sounds.\n",
      "\n",
      "4. **Scene Relationship Diagram:**\n",
      "   - Little Boy: Playing with a toy, wearing a tie\n",
      "   - Young Child: Playing with a toy\n",
      "   - Mother: Taking care of both children\n",
      "   - Interaction: The young child turns around and does something typical for a young child.\n",
      "\n",
      "5. **JSON Format:**\n",
      "```json\n",
      "{\n",
      "  \"scene\": {\n",
      "    \"characters\": [\n",
      "      {\n",
      "        \"name\": \"Little Boy\",\n",
      "        \"action\": \"Playing with a toy, wearing a tie\"\n",
      "      },\n",
      "      {\n",
      "        \"name\": \"Young Child\",\n",
      "        \"action\": \"Playing with a toy\"\n",
      "      },\n",
      "      {\n",
      "        \"name\": \"Mother\",\n",
      "        \"action\": \"Taking care of both children\"\n",
      "      }\n",
      "    ],\n",
      "    \"interaction\": {\n",
      "      \"description\": \"The young child turns around and engages in a typical child behavior such as looking at something new, reaching for a toy, or making sounds.\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "```\n",
      "\n",
      "This JSON format encapsulates the characters, their actions, and the inferred interaction based on the given scenario.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Give me a short introduction to large language model.\"\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"你只能用英语回复，你是一个场景内容推理器，请基于场景和内容，给出推理过程，给出场景关系图，给出json格式。\"},\n",
    "    {\"role\": \"user\", \"content\": \"The description is：There is a little boy, a young child and his mom. The little boy is playing with a toy and wearing a tie. The young child is also playing with a toy. And, the mom is taking care of the little boy and young child. what did the baby do after he turned around near the end of the video？ \"},\n",
    "]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "import torch\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "generated_ids = model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=1024\n",
    ")\n",
    "generated_ids = [\n",
    "    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "]\n",
    "\n",
    "response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "print(response)\n",
    "\n",
    "{\"role\": \"assistant\", \"content\": f\"The question is {question}. The options are {A B C D E}. 给出选择这个选项的可能的概率，从1到10给出评分，10为最有可能，1为没有可能。格式为【1/10】\"},"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = response\n",
    "import re\n",
    "import json\n",
    "flag = 0\n",
    "# text = text2\n",
    "try:\n",
    "    json_pattern = r\"'''json\\s*(\\{(?:.|\\n)*?\\})\\s*'''\"\n",
    "    matches = re.findall(json_pattern, text)\n",
    "    # print(matches)\n",
    "    # 假设我们只关心第一个匹配项\n",
    "    json_data = json.loads(matches[0]) if matches else None\n",
    "    print(json_data)\n",
    "except:\n",
    "    flag = 1\n",
    "if flag:\n",
    "  try:\n",
    "      json_pattern = r'\\{.*\\}'\n",
    "      matches = re.findall(json_pattern, text2, re.DOTALL)\n",
    "\n",
    "      # 假设我们只关心第一个匹配项\n",
    "      json_data = json.loads(matches[0]) if matches else None\n",
    "      print(json_data)\n",
    "  except:\n",
    "      flag = 101"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_entity(prompt, question):\n",
    "    sss = \"\"\"'''json\n",
    "    {\n",
    "    \"entities\": [\n",
    "        {\n",
    "        \"text\": \"baby\",\n",
    "        \"type\": \"PERSON\"\n",
    "        }\n",
    "    ]\n",
    "    }\n",
    "    '''\"\"\"\n",
    "        \n",
    "    prompt = \"Give me a short introduction to large language model.\"\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"你是一个实体抽取和句法依存分析器，请基于问题，给出场景中的实体，并和题目中的实体进行匹配，给出实体内容，以问句中的实体为准，给出对应的json格式。你只能给出实体，格式如下：\"+sss},\n",
    "        {\"role\": \"assistant\", \"content\": question},\n",
    "        {\"role\": \"user\", \"content\": \"The scenario is：\"+prompt},\n",
    "    ]\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    generated_ids = model.generate(\n",
    "        **model_inputs,\n",
    "        max_new_tokens=512\n",
    "    )\n",
    "    generated_ids = [\n",
    "        output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "    ]\n",
    "\n",
    "    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "    print('step 1: entity')\n",
    "    print(response)\n",
    "\n",
    "    return response\n",
    "\n",
    "def generate_scene_graph(prompt, question):\n",
    "    messages = [\n",
    "    {\"role\": \"system\", \"content\": \"你只能用英语回复，你是一个场景内容推理器，请基于场景和内容，给出推理过程，给出场景关系图，给出json格式。\"},\n",
    "    {\"role\": \"user\", \"content\": \"The description is：There is a little boy, a young child and his mom. The little boy is playing with a toy and wearing a tie. The young child is also playing with a toy. And, the mom is taking care of the little boy and young child. what did the baby do after he turned around near the end of the video？ \"},\n",
    "]\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    generated_ids = model.generate(\n",
    "        **model_inputs,\n",
    "        max_new_tokens=1024\n",
    "    )\n",
    "    generated_ids = [\n",
    "        output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "    ]\n",
    "\n",
    "    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "\n",
    "    text = response\n",
    "    import re\n",
    "    import json\n",
    "    flag = 0\n",
    "    # text = text2\n",
    "    try:\n",
    "        json_pattern = r\"'''json\\s*(\\{(?:.|\\n)*?\\})\\s*'''\"\n",
    "        matches = re.findall(json_pattern, text)\n",
    "        # print(matches)\n",
    "        # 假设我们只关心第一个匹配项\n",
    "        json_data = json.loads(matches[0]) if matches else None\n",
    "        print(json_data)\n",
    "    except:\n",
    "        flag = 1\n",
    "        if flag:\n",
    "            try:\n",
    "                json_pattern = r'\\{.*\\}'\n",
    "                matches = re.findall(json_pattern, text, re.DOTALL)\n",
    "\n",
    "                # 假设我们只关心第一个匹配项\n",
    "                json_data = json.loads(matches[0]) if matches else None\n",
    "                print(json_data)\n",
    "            except:\n",
    "                flag = 101\n",
    "    print('step 2: scene graph')\n",
    "    print(json_data)\n",
    "    return json_data\n",
    "\n",
    "def generate_enhancement_scene(prompt, question, fact):\n",
    "    # prompt = \"Give me a short introduction to large language model.\"\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"你是一个场景增强器，你需要根据我的问题和我给你的答案还有增强场景描述，将这句话扩展到你生成的描述中。\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"用英语回答，只能生成一段话\"},\n",
    "        {\"role\": \"user\", \"content\": \"The scenario is：\"+prompt+\"The question is\"+question+\"The answer (fact) is\"+fact},\n",
    "    ]\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    generated_ids = model.generate(\n",
    "        **model_inputs,\n",
    "        max_new_tokens=512\n",
    "    )\n",
    "    generated_ids = [\n",
    "        output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "    ]\n",
    "\n",
    "    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "    print('step 1: entity')\n",
    "    print(response)\n",
    "\n",
    "    return response\n",
    "\n",
    "def generate_question_rank(prompt, question, fact):\n",
    "    # prompt = \"Give me a short introduction to large language model.\"\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"你是一个题目与场景比对器，根据我给你的场景描述，一步一步分析.\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"用英语回答，从0-10进行打分，给出你认为的相关性，完全合理赋10分，完全不合理赋0分。输出格式：[5]，你的回答是什么。\"},\n",
    "        {\"role\": \"user\", \"content\": \"The scenario is：\"+prompt+\"The question is\"+question+\". The answer is \"+fact},\n",
    "    ]\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    generated_ids = model.generate(\n",
    "        **model_inputs,\n",
    "        max_new_tokens=512\n",
    "    )\n",
    "    generated_ids = [\n",
    "        output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "    ]\n",
    "\n",
    "    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "    print('step 3: score')\n",
    "    print(response)\n",
    "    \n",
    "    return response\n",
    "\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "构建scene graph：\n",
      "- 主体1（person1）：小男孩（little boy），特征：年轻，正在玩玩具，穿着领结\n",
      "- 主体2（person2）：年幼的孩子（young child），特征：年轻，正在玩玩具\n",
      "- 主体3（person3）：妈妈（mom），特征：在照顾小男孩和年幼的孩子\n",
      "- 物品1（object1）：玩具（toy）\n",
      "- 物品2（object2）：领结（tie）\n",
      "\n",
      "分析选项并评分：\n",
      "\n",
      "a) hug boy (拥抱男孩)\n",
      "   - 分析：视频中没有提到小男孩被其他人拥抱，也没有任何信息表明他会去拥抱别人。\n",
      "   - 评分：1\n",
      "\n",
      "b) flail around (乱动)\n",
      "   - 分析：视频中没有明确说明小男孩是否会有这种行为，但考虑到他正在玩玩具，有可能会出现这种情况。\n",
      "   - 评分：4\n",
      "\n",
      "c) touch the camera (触碰相机)\n",
      "   - 分析：场景描述中并没有提到相机的存在，因此这个选项不符合场景描述。\n",
      "   - 评分：1\n",
      "\n",
      "d) push box to where he started (把箱子推到他开始的地方)\n",
      "   - 分析：场景描述中并没有提到箱子，所以这个选项也不符合场景描述。\n",
      "   - 评分：1\n",
      "\n",
      "e) fall (摔倒)\n",
      "   - 分析：视频中没有提到小男孩摔倒的情况，也没有任何信息表明他会这样做。\n",
      "   - 评分：1\n",
      "\n",
      "最终答案：根据以上分析，最符合场景描述的选项是b) flail around (乱动)，评分为4。然而，需要注意的是，这只是一个推测性的结论，因为题干中的描述并不完整，无法完全确定小男孩的行为。\n"
     ]
    }
   ],
   "source": [
    "from modelscope import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name = \"/sdb/ccfa/video_understanding/Qwen2.5-7B-Instruct\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"cuda:0\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# history\n",
    "question = \"what did the baby do after he turned around near the end of the video？\"\n",
    "prompt = \"There is a little boy, a young child and his mom. The little boy is playing with a toy and wearing a tie. The young child is also playing with a toy. And, the mom is taking care of the little boy and young child.\"\n",
    "fact = \"a) hug boy\tb) flail around\tc) touch the camera\td) push box to where he started\te) fall\"\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"你是一个内容理解与推理专家，你需要做的是从我的场景描述中进行内容理解和推理，请基于问题，推断出场景，并回答我的问题。\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"请回答我的问题：The question is\"+question+\". The options is \"+fact+\". 回答格式：【选项k)】\"},\n",
    "    {\"role\": \"user\", \"content\": \"请根据我给你的内容一步一步考虑：1) 构建scene graph 2) 对每个选项进行分析，对每个选项打一个1-10的评分。3）给出最终的答案。The scenario is：\"+prompt}\n",
    "]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "generated_ids = model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=512\n",
    ")\n",
    "generated_ids = [\n",
    "    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "]\n",
    "\n",
    "response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "# print('step 3: score')\n",
    "print(response)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enhance(prompt, fact):\n",
    "    question = \"what did the baby do after he turned around near the end of the video？\"\n",
    "    prompt = \"There is a little boy, a young child and his mom. The little boy is playing with a toy and wearing a tie. The young child is also playing with a toy. And, the mom is taking care of the little boy and young child.\"\n",
    "    fact = \"a) hug boy\tb) flail around\tc) touch the camera\td) push box to where he started\te) fall\"\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"你是一个内容增强专家，你需要根据我给你的事实（问答题），对场景介绍进行增强，并最后同样生成一段话。\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"question: \"+question+\". answer: \"+fact+\". 请生成一段内容增强的话。\"},\n",
    "        {\"role\": \"user\", \"content\": \"The scenario is：\"+prompt}\n",
    "    ]\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    generated_ids = model.generate(\n",
    "        **model_inputs,\n",
    "        max_new_tokens=512\n",
    "    )\n",
    "    generated_ids = [\n",
    "        output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "    ]\n",
    "\n",
    "    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "    # print('step 3: score')\n",
    "    print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In this heartwarming scene, a little boy, adorned in a charming tie, is joyfully playing with a colorful toy while his attentive mother keeps a watchful eye on him. Nearby, his younger sibling is equally engrossed in their own playtime with a delightful toy. As the video progresses, the little boy, who has been sitting contentedly, decides to turn around, only for the camera to capture a moment of pure, innocent delight – and a gentle reminder of childhood antics – as he falls softly to the ground, all the while his mother's loving gaze never straying far.\n"
     ]
    }
   ],
   "source": [
    "# question = \"what did the baby do after he turned around near the end of the video？\"\n",
    "# prompt = \"There is a little boy, a young child and his mom. The little boy is playing with a toy and wearing a tie. The young child is also playing with a toy. And, the mom is taking care of the little boy and young child.\"\n",
    "# fact = \"e) fall\"\n",
    "# messages = [\n",
    "#     {\"role\": \"system\", \"content\": \"你是一个内容增强专家，你需要根据我给你的事实（问答题），对场景介绍进行增强，并最后同样生成一段话。\"},\n",
    "#     {\"role\": \"assistant\", \"content\": \"question: \"+question+\". answer: \"+fact+\". 请生成一段内容增强的话。\"},\n",
    "#     {\"role\": \"user\", \"content\": \"The scenario is：\"+prompt}\n",
    "# ]\n",
    "# text = tokenizer.apply_chat_template(\n",
    "#     messages,\n",
    "#     tokenize=False,\n",
    "#     add_generation_prompt=True\n",
    "# )\n",
    "# model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "# generated_ids = model.generate(\n",
    "#     **model_inputs,\n",
    "#     max_new_tokens=512\n",
    "# )\n",
    "# generated_ids = [\n",
    "#     output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "# ]\n",
    "\n",
    "# response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "# # print('step 3: score')\n",
    "# print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def response_fact(prompt, question, fact):\n",
    "    question = \"what did the baby do after he turned around near the end of the video？\"\n",
    "    prompt = \"There is a little boy, a young child and his mom. The little boy is playing with a toy and wearing a tie. The young child is also playing with a toy. And, the mom is taking care of the little boy and young child.\"\n",
    "    fact = \"a) hug boy\tb) flail around\tc) touch the camera\td) push box to where he started\te) fall\"\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"你是一个内容理解与推理专家，你需要做的是从我的场景描述中进行内容理解和推理，请基于问题，推断出场景，并回答我的问题。\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"请回答我的问题：The question is\"+question+\". The options is \"+fact+\". 回答格式：【选项k)】\"},\n",
    "        {\"role\": \"user\", \"content\": \"请根据我给你的内容一步一步考虑：1) 构建scene graph 2) 对每个选项进行分析，对每个选项打一个1-10的评分。3）给出最终的答案。The scenario is：\"+prompt}\n",
    "    ]\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    generated_ids = model.generate(\n",
    "        **model_inputs,\n",
    "        max_new_tokens=512\n",
    "    )\n",
    "    generated_ids = [\n",
    "        output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "    ]\n",
    "\n",
    "    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "    # print('step 3: score')\n",
    "    print(response)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
